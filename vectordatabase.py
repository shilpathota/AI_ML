# -*- coding: utf-8 -*-
"""VectorDatabase.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tRjjz1o1NSd0ePq5T9-hHiPAkGMyycSx

# Vector Database in 6 ways

## Vector Database for Semantic Search



*   Get the Quora Dataset and create embeddings


*   Store it in Pinecone
- Extract the questions which are similar and have same semantic meaning
"""

!pip install python-dotenv numpy huggingface_hub pandas scikit-learn sentence-transformers matplotlib torch langchain openai pinecone pinecone-datasets pinecone-text tiktoken tqdm datasets deepface

!pip install -U datasets
!pip install -U sentence-transformers
!pip uninstall -y numpy
!pip install numpy --upgrade --force-reinstall
import os
os.kill(os.getpid(), 9)  # ðŸ”„ Force runtime restart

import warnings
warnings.filterwarnings('ignore')

from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone, ServerlessSpec
from DLAIUtils import Utils
import DLAIUtils

import os
import time
import torch

from tqdm.auto import tqdm

"""### Loading the dataset"""

!rm -rf ~/.cache/huggingface/datasets/quora

import os
os.environ["HF_DATASETS_CACHE"] = "/tmp/hf-datasets-cache"

from datasets import load_dataset

# Load full dataset fresh (after clearing cache)
full_dataset = load_dataset("quora", split="train")

# Slice from index 240000 to 289999 (50K samples)
dataset = full_dataset.select(range(240000, 290000))

dataset[:5]

questions = []
for record in dataset['questions']:
    questions.extend(record['text'])
question = list(set(questions))
print('\n'.join(questions[:10]))
print('-' * 50)
print(f'Number of questions: {len(questions)}')

device = 'cuda' if torch.cuda.is_available() else 'cpu'
if device != 'cuda':
    print('Sorry no cuda.')
model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

query = 'which city is the most populated in the world?'
xq = model.encode(query)
xq.shape

utils = Utils()
import getpass
PINECONE_API_KEY = getpass.getpass('Pinecone API Key:')

pinecone = Pinecone(api_key=PINECONE_API_KEY)
INDEX_NAME = utils.create_dlai_index_name('dl-ai')

if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:
    pinecone.delete_index(INDEX_NAME)
print(INDEX_NAME)
pinecone.create_index(name=INDEX_NAME,
    dimension=model.get_sentence_embedding_dimension(),
    metric='cosine',
    spec=ServerlessSpec(cloud='aws', region='us-east-1'))

index = pinecone.Index(INDEX_NAME)
print(index)

"""Creating Embeddings and Upsert to Pinecone"""

batch_size=200
vector_limit=10000

questions = question[:vector_limit]

import json

for i in tqdm(range(0, len(questions), batch_size)):
    # find end of batch
    i_end = min(i+batch_size, len(questions))
    # create IDs batch
    ids = [str(x) for x in range(i, i_end)]
    # create metadata batch
    metadatas = [{'text': text} for text in questions[i:i_end]]
    # create embeddings
    xc = model.encode(questions[i:i_end])
    # create records list for upsert
    records = zip(ids, xc, metadatas)
    # upsert to Pinecone
    index.upsert(vectors=records)

index.describe_index_stats()

"""Run your Query"""

# small helper function so we can repeat queries later
def run_query(query):
  embedding = model.encode(query).tolist()
  results = index.query(top_k=10, vector=embedding, include_metadata=True, include_values=False)
  for result in results['matches']:
    print(f"{round(result['score'], 2)}: {result['metadata']['text']}")

run_query('which city has the highest population in the world?')

query = 'how do i make chocolate cake?'
run_query(query)

"""## Vector Database for RAG
 - The query is sent to Pinecone
 - We get the responses from pine cone
 - Create a prompt out of responses
 - Send it to open AI
"""

import warnings
warnings.filterwarnings('ignore')

from datasets import load_dataset
from openai import OpenAI
from pinecone import Pinecone, ServerlessSpec
from tqdm.auto import tqdm
from DLAIUtils import Utils

import ast
import os
import pandas as pd

pinecone = Pinecone(api_key=PINECONE_API_KEY)
INDEX_NAME = utils.create_dlai_index_name('dl-ai')

if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:
    pinecone.delete_index(INDEX_NAME)
print(INDEX_NAME)
pinecone.create_index(name=INDEX_NAME,
    dimension=384,
    metric='cosine',
    spec=ServerlessSpec(cloud='aws', region='us-east-1'))

index = pinecone.Index(INDEX_NAME)
print(index)

"""### Load the Dataset"""

!wget -q -O lesson2-wiki.csv.zip "https://www.dropbox.com/scl/fi/yxzmsrv2sgl249zcspeqb/lesson2-wiki.csv.zip?rlkey=paehnoxjl3s5x53d1bedt4pmc&dl=0"

!unzip lesson2-wiki.csv.zip

max_articles_num = 500
df = pd.read_csv('./wiki.csv', nrows=max_articles_num)
df.head()

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

"""### Prepare the Embeddings and Upsert to pinecone"""

prepped = []
print(df.columns)


for i, row in tqdm(df.iterrows(), total=df.shape[0]):
    meta = ast.literal_eval(row['metadata'])  # If this is valid JSON
    text = row['values']  # âœ… Replace with your actual column name (e.g., 'text', 'title', etc.)

    # Generate 384-dim embedding
    vector = model.encode(text).tolist()

    prepped.append({
        'id': row['id'],
        'values': vector,
        'metadata': meta
    })

    if len(prepped) >= 250:
        index.upsert(prepped)
        prepped = []

# Final batch
if prepped:
    index.upsert(prepped)

index.describe_index_stats()

"""Connect to OpenAI"""

from google.colab import userdata
OPENAI_API_KEY = userdata.get("OPENAI_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY)

#def get_embeddings(articles, model="text-embedding-ada-002"):
#   return openai_client.embeddings.create(input = articles, model=model)

from sentence_transformers import SentenceTransformer

local_model = SentenceTransformer('all-MiniLM-L6-v2')

def get_embeddings(articles):
    return local_model.encode(articles, convert_to_numpy=True)

"""Run your Query"""

query = "what is the berlin wall?"

# Get 384-dim embedding directly
embed = model.encode(query).tolist()

# Query Pinecone with the local embedding
res = index.query(vector=embed, top_k=3, include_metadata=True)

# Extract and print the results
text = [match['metadata']['text'] for match in res['matches']]
print('\n'.join(text))

"""Build the Prompt"""

from sentence_transformers import SentenceTransformer

# Load your local embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Your input query
query = "write an article titled: what is the berlin wall?"

# Step 1: Get local embedding
embed = model.encode(query).tolist()  # 384-dim vector

# Step 2: Query Pinecone with the vector
res = index.query(vector=embed, top_k=3, include_metadata=True)

# Step 3: Extract matched text from metadata
contexts = [match['metadata']['text'] for match in res['matches']]

# Step 4: Build prompt
prompt_start = (
    "Answer the question based on the context below.\n\n"
    "Context:\n"
)
prompt_end = (
    f"\n\nQuestion: {query}\nAnswer:"
)
prompt = prompt_start + "\n\n---\n\n".join(contexts) + prompt_end

# Step 5: Print the final prompt
print(prompt)

"""Get the summary"""

res = openai_client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ],
    temperature=0,
    max_tokens=636,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
)
print('-' * 80)
print(res.choices[0].message.content)

!pip install -q huggingface_hub
from huggingface_hub import login

HF_API_KEY = userdata.get("HF_API_KEY")
login(token=HF_API_KEY)

from huggingface_hub import InferenceClient

client = InferenceClient(model="HuggingFaceH4/zephyr-7b-beta")

output = client.text_generation(prompt=prompt, max_new_tokens=300)

print(output)

"""## Vector Database for Recommender Systems

- Convert News Article Titles to embeddingsn and save in Pinecone
- Pass the query to the Pinecone
- Gives all the related articles with titles in the news
- gets the news articles and put the embeddings
- Pass the query to pinecone
- Gets all the related articles with content in the news
"""

import warnings
warnings.filterwarnings('ignore')

from langchain.text_splitter import RecursiveCharacterTextSplitter
from openai import OpenAI
from pinecone import Pinecone, ServerlessSpec
from tqdm.auto import tqdm, trange
from DLAIUtils import Utils

import pandas as pd
import time
import os

"""### Loading the dataset"""

!wget -q --show-progress -O all-the-news-3.zip "https://www.dropbox.com/scl/fi/wruzj2bwyg743d0jzd7ku/all-the-news-3.zip?rlkey=rgwtwpeznbdadpv3f01sznwxa&dl=1"

!unzip all-the-news-3.zip

with open('./all-the-news-3.csv', 'r') as f:
    header = f.readline()
    print(header)

df = pd.read_csv('./all-the-news-3.csv', nrows=99)
df.head()

"""Setting up the Pinecone"""

openai_client = OpenAI(api_key=OPENAI_API_KEY)
util = Utils()
INDEX_NAME = utils.create_dlai_index_name('dl-ai')
pinecone = Pinecone(api_key=PINECONE_API_KEY)

if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:
  pinecone.delete_index(INDEX_NAME)

pinecone.create_index(name=INDEX_NAME, dimension=model.get_sentence_embedding_dimension(), metric='cosine',
  spec=ServerlessSpec(cloud='aws', region='us-east-1'))

index = pinecone.Index(INDEX_NAME)

"""Create Embeddings of the News Titles"""

def get_embeddings(articles, model_name="all-MiniLM-L6-v2"):
    model = SentenceTransformer(model_name)
    return model.encode(articles, convert_to_tensor=False)

CHUNK_SIZE=400
TOTAL_ROWS=10000
progress_bar = tqdm(total=TOTAL_ROWS)
chunks = pd.read_csv('./all-the-news-3.csv', chunksize=CHUNK_SIZE,
                     nrows=TOTAL_ROWS)
chunk_num = 0
for chunk in chunks:
    titles = chunk['title'].tolist()
    embeddings = get_embeddings(titles)
    prepped = [{'id': str(chunk_num * CHUNK_SIZE + i),
            'values': embeddings[i],
            'metadata': {'title': titles[i]}} for i in range(len(titles))]
    chunk_num = chunk_num + 1
    if len(prepped) >= 200:
      index.upsert(prepped)
      prepped = []
    progress_bar.update(len(chunk))

index.describe_index_stats()

"""Build the Recommender System"""

def get_recommendations(pinecone_index, search_term, top_k=10):
    embed = get_embeddings([search_term])[0].tolist()  # Convert from ndarray to list
    res = pinecone_index.query(vector=embed, top_k=top_k, include_metadata=True)
    return res

reco = get_recommendations(index, 'obama')
for r in reco.matches:
    print(f'{r.score:.4f} : {r.metadata["title"]}')

"""Create Embeddings of All News Content"""

if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:
  pinecone.delete_index(name=INDEX_NAME)

pinecone.create_index(name=INDEX_NAME, dimension=model.get_sentence_embedding_dimension(), metric='cosine',
  spec=ServerlessSpec(cloud='aws', region='us-east-1'))
articles_index = pinecone.Index(INDEX_NAME)

def embed(embeddings, title, prepped, embed_num):
    for embedding in embeddings:
        prepped.append({
            'id': str(embed_num),
            'values': embedding.tolist(),  # Convert from ndarray to plain list
            'metadata': {'title': title}
        })
        embed_num += 1
        if len(prepped) >= 100:
            articles_index.upsert(prepped)
            prepped.clear()
    return embed_num

news_data_rows_num = 100

embed_num = 0 #keep track of embedding number for 'id'
text_splitter = RecursiveCharacterTextSplitter(chunk_size=400,
    chunk_overlap=20) # how to chunk each article
prepped = []
df = pd.read_csv('./all-the-news-3.csv', nrows=news_data_rows_num)
articles_list = df['article'].tolist()
titles_list = df['title'].tolist()

for i in range(len(articles_list)):
    print(".", end="")
    article = articles_list[i]
    title = titles_list[i]
    if isinstance(article, str) and article.strip():
        chunks = text_splitter.split_text(article)
        embeddings = get_embeddings(chunks)
        embed_num = embed(embeddings, title, prepped, embed_num)

articles_index.describe_index_stats()

"""Build the Recommender System"""

reco = get_recommendations(articles_index, 'obama', top_k=100)
seen = {}
for r in reco.matches:
    title = r.metadata['title']
    if title not in seen:
        print(f'{r.score} : {title}')
        seen[title] = '.'

"""## Vector Databse for Hybrid Search"""

import warnings
warnings.filterwarnings('ignore')

from datasets import load_dataset
from pinecone_text.sparse import BM25Encoder
from pinecone import Pinecone, ServerlessSpec
from DLAIUtils import Utils

from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm
import torch
import os

utils = Utils()
PINECONE_API_KEY = utils.get_pinecone_api_key()

"""Setup Pinecone"""

from google.colab import userdata
PINECONE_API_KEY = userdata.get("PINECONE_API_KEY")
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

utils = Utils()
INDEX_NAME = utils.create_dlai_index_name('dl-ai')
pinecone = Pinecone(api_key=PINECONE_API_KEY)

if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:
  pinecone.delete_index(INDEX_NAME)
pinecone.create_index(
  INDEX_NAME,
  dimension=512,
  metric="dotproduct",
  spec=ServerlessSpec(cloud='aws', region='us-east-1')
)
index = pinecone.Index(INDEX_NAME)

"""Load the dataset"""

fashion = load_dataset(
    "ashraq/fashion-product-images-small",
    split="train"
)
fashion

images = fashion['image']
metadata = fashion.remove_columns('image')
images[900]

metadata = metadata.to_pandas()
metadata.head()

"""Create the Sparse Vector using BM25"""

bm25 = BM25Encoder()
bm25.fit(metadata['productDisplayName'])
metadata['productDisplayName'][0]

bm25.encode_queries(metadata['productDisplayName'][0])
bm25.encode_documents(metadata['productDisplayName'][0])

"""Create the Dense Vector Using CLIP"""

model = SentenceTransformer('sentence-transformers/clip-ViT-B-32',
    device=device)
model
dense_vec = model.encode([metadata['productDisplayName'][0]])
dense_vec.shape

len(fashion)

"""Create Embeddings using Sparse and Dense"""

batch_size = 100
fashion_data_num = 1000

for i in tqdm(range(0, min(fashion_data_num,len(fashion)), batch_size)):
    # find end of batch
    i_end = min(i+batch_size, len(fashion))
    # extract metadata batch
    meta_batch = metadata.iloc[i:i_end]
    meta_dict = meta_batch.to_dict(orient="records")
    # concatinate all metadata field except for id and year to form a single string
    meta_batch = [" ".join(x) for x in meta_batch.loc[:, ~meta_batch.columns.isin(['id', 'year'])].values.tolist()]
    # extract image batch
    img_batch = images[i:i_end]
    # create sparse BM25 vectors
    sparse_embeds = bm25.encode_documents([text for text in meta_batch])
    # create dense vectors
    dense_embeds = model.encode(img_batch).tolist()
    # create unique IDs
    ids = [str(x) for x in range(i, i_end)]

    upserts = []
    # loop through the data and create dictionaries for uploading documents to pinecone index
    for _id, sparse, dense, meta in zip(ids, sparse_embeds, dense_embeds, meta_dict):
        upserts.append({
            'id': _id,
            'sparse_values': sparse,
            'values': dense,
            'metadata': meta
        })
    # upload the documents to the new hybrid index
    index.upsert(upserts)

# show index description after uploading the documents
index.describe_index_stats()

"""Run your Query"""

query = "dark blue french connection jeans for men"

sparse = bm25.encode_queries(query)
dense = model.encode(query).tolist()

result = index.query(
    top_k=14,
    vector=dense,
    sparse_vector=sparse,
    include_metadata=True
)

imgs = [images[int(r["id"])] for r in result["matches"]]
imgs

from IPython.core.display import HTML
from io import BytesIO
from base64 import b64encode

# function to display product images
def display_result(image_batch):
    figures = []
    for img in image_batch:
        b = BytesIO()
        img.save(b, format='png')
        figures.append(f'''
            <figure style="margin: 5px !important;">
              <img src="data:image/png;base64,{b64encode(b.getvalue()).decode('utf-8')}" style="width: 90px; height: 120px" >
            </figure>
        ''')
    return HTML(data=f'''
        <div style="display: flex; flex-flow: row wrap; text-align: center;">
        {''.join(figures)}
        </div>
    ''')

display_result(imgs)

"""Scaling the Hybrid Search"""

def hybrid_scale(dense, sparse, alpha: float):
    """Hybrid vector scaling using a convex combination

    alpha * dense + (1 - alpha) * sparse

    Args:
        dense: Array of floats representing
        sparse: a dict of `indices` and `values`
        alpha: float between 0 and 1 where 0 == sparse only
               and 1 == dense only
    """
    if alpha < 0 or alpha > 1:
        raise ValueError("Alpha must be between 0 and 1")
    # scale sparse and dense vectors to create hybrid search vecs
    hsparse = {
        'indices': sparse['indices'],
        'values':  [v * (1 - alpha) for v in sparse['values']]
    }
    hdense = [v * alpha for v in dense]
    return hdense, hsparse

"""More Dense"""

question = "dark blue french connection jeans for men"
#Closer to 0==more sparse, closer to 1==more dense
hdense, hsparse = hybrid_scale(dense, sparse, alpha=1)
result = index.query(
    top_k=6,
    vector=hdense,
    sparse_vector=hsparse,
    include_metadata=True
)
imgs = [images[int(r["id"])] for r in result["matches"]]
display_result(imgs)

for x in result["matches"]:
    print(x["metadata"]['productDisplayName'])

"""More Sparse"""

question = "dark blue french connection jeans for men"
#Closer to 0==more sparse, closer to 1==more dense
hdense, hsparse = hybrid_scale(dense, sparse, alpha=0)
result = index.query(
    top_k=6,
    vector=hdense,
    sparse_vector=hsparse,
    include_metadata=True
)
imgs = [images[int(r["id"])] for r in result["matches"]]
display_result(imgs)

for x in result["matches"]:
    print(x["metadata"]['productDisplayName'])

"""More Dense or More sparse?"""

question = "dark blue french connection jeans for men"
#Closer to 0==more sparse, closer to 1==more dense
hdense, hsparse = hybrid_scale(dense, sparse, alpha=1)
result = index.query(
    top_k=6,
    vector=hdense,
    sparse_vector=hsparse,
    include_metadata=True
)
imgs = [images[int(r["id"])] for r in result["matches"]]
display_result(imgs)

for x in result["matches"]:
    print(x["metadata"]['productDisplayName'])

"""## Vector Database for Facial Similarity Search"""

import warnings
warnings.filterwarnings('ignore')

from deepface import DeepFace
from pinecone import Pinecone, ServerlessSpec
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from tqdm import tqdm
from DLAIUtils import Utils


import contextlib
import glob
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import time

# get api key
utils = Utils()
PINECONE_API_KEY = utils.get_pinecone_api_key()

"""Load the Dataset"""

!wget -q --show-progress -O family_photos.zip "https://www.dropbox.com/scl/fi/yg0f2ynbzzd2q4nsweti5/family_photos.zip?rlkey=00oeuiii3jgapz2b1bfj0vzys&dl=0"

!unzip -q family_photos.zip

def show_img(f):
  img = plt.imread(f)
  plt.figure(figsize=(4,3))
  plt.imshow(img)

show_img('family/dad/P06260_face5.jpg')

show_img('family/mom/P04407_face2.jpg')

show_img('family/child/P04414_face1.jpg')

"""Setup Pinecone"""

from google.colab import userdata
PINECONE_API_KEY = userdata.get("PINECONE_API_KEY")
MODEL = "Facenet"
INDEX_NAME = utils.create_dlai_index_name('dl-ai')

pinecone = Pinecone(api_key=PINECONE_API_KEY)

"""Creating Embeddings using DeepFace"""

def generate_vectors():
  VECTOR_FILE = "./vectors.vec"

  with contextlib.suppress(FileNotFoundError):
    os.remove(VECTOR_FILE)
  with open(VECTOR_FILE, "w") as f:
    for person in ["mom", "dad", "child"]:
      files = glob.glob(f'family/{person}/*')
      for file in tqdm(files):
        try:
          embedding = DeepFace.represent(img_path=file, model_name=MODEL, enforce_detection=False)[0]['embedding']
          f.write(f'{person}:{os.path.basename(file)}:{embedding}\n')
        except (ValueError, UnboundLocalError, AttributeError) as e:
          print(e)

generate_vectors()

!head -10 vectors.vec

"""Plot the data of images"""

def gen_tsne_df(person, perplexity):
    vectors =[]
    with open('./vectors.vec', 'r') as f:
      for line in tqdm(f):
        p, orig_img, v = line.split(':')
        if person == p:
            vectors.append(eval(v))
    pca = PCA(n_components=8)
    tsne = TSNE(2, perplexity=perplexity, random_state = 0, n_iter=1000,
        verbose=0, metric='euclidean', learning_rate=75)
    print(f'transform {len(vectors)} vectors')
    pca_transform = pca.fit_transform(vectors)
    embeddings2d = tsne.fit_transform(pca_transform)
    return pd.DataFrame({'x':embeddings2d[:,0], 'y':embeddings2d[:,1]})

def plot_tsne(perplexity, model):
    (_, ax) = plt.subplots(figsize=(8,5))
    #plt.style.use('seaborn-whitegrid')
    plt.grid(color='#EAEAEB', linewidth=0.5)
    ax.spines['top'].set_color(None)
    ax.spines['right'].set_color(None)
    ax.spines['left'].set_color('#2B2F30')
    ax.spines['bottom'].set_color('#2B2F30')
    colormap = {'dad':'#ee8933', 'child':'#4fad5b', 'mom':'#4c93db'}

    for person in colormap:
        embeddingsdf = gen_tsne_df(person, perplexity)
        ax.scatter(embeddingsdf.x, embeddingsdf.y, alpha=.5,
                   label=person, color=colormap[person])
    plt.title(f'Scatter plot of faces using {model}', fontsize=16, fontweight='bold', pad=20)
    plt.suptitle(f't-SNE [perplexity={perplexity}]', y=0.92, fontsize=13)
    plt.legend(loc='best', frameon=True)
    plt.show()

plot_tsne(44, 'facenet')

"""Store the embeddings in pinecone"""

if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:
  pinecone.delete_index(INDEX_NAME)
pinecone.create_index(name=INDEX_NAME, dimension=128, metric='cosine',
  spec=ServerlessSpec(cloud='aws', region='us-east-1'))

index = pinecone.Index(INDEX_NAME)

def store_vectors():
  with open("vectors.vec", "r") as f:
    for line in tqdm(f):
        person, file, vec = line.split(':')
        index.upsert([(f'{person}-{file}', eval(vec), {"person":person, "file":file})])
store_vectors()

index.describe_index_stats()

"""Calculate the Similarity Scores"""

def test(vec_groups, parent, child):
  index = pinecone.Index(INDEX_NAME)
  parent_vecs = vec_groups[parent]
  K = 10
  SAMPLE_SIZE = 10
  sum = 0
  for i in tqdm(range(0,SAMPLE_SIZE)):
    query_response = index.query(
      top_k=K,
      vector = parent_vecs[i],
      filter={
        "person": {"$eq": child}
      }
    )
    for row in query_response["matches"]:
      sum  = sum + row["score"]
  print(f'\n\n{parent} AVG: {sum / (SAMPLE_SIZE*K)}')

def compute_scores():
  index = pinecone.Index(INDEX_NAME)
  vec_groups = {"dad":[], "mom":[], "child":[]}
  with open("vectors.vec", "r") as f:
    for line in tqdm(f):
      person, file, vec = line.split(':')
      vec_groups[person].append(eval(vec))
  print(f"DAD {'-' * 20}")
  test(vec_groups, "dad", "child")
  print(f"MOM {'-' * 20}")
  test(vec_groups, "mom", "child")

compute_scores()

"""Check the matching images"""

child_base = 'family/child/P06310_face1.jpg'
show_img(child_base)

#Now find closest given we know dad is "most similar"
embedding = DeepFace.represent(img_path=child_base, model_name=MODEL)[0]['embedding']
print(embedding)

query_response = index.query(
      top_k=3,
      vector = embedding,
      filter={
        "person": {"$eq": "dad"}
      },
      include_metadata=True
)

print(query_response)

photo = query_response['matches'][0]['metadata']['file']

show_img(f'family/dad/{photo}')

"""## Vector Databases for Anomaly Detection"""

import warnings
warnings.filterwarnings('ignore')

from pinecone import Pinecone, ServerlessSpec
from sentence_transformers import SentenceTransformer, InputExample, losses, models, util
from torch.utils.data import DataLoader
from torch import nn
from tqdm.auto import tqdm
from DLAIUtils import Utils
import torch
import time
import torch
import os

"""Setup Pinecone"""

utils = Utils()
PINECONE_API_KEY = utils.get_pinecone_api_key()

from google.colab import userdata
PINECONE_API_KEY = userdata.get("PINECONE_API_KEY")

INDEX_NAME = utils.create_dlai_index_name('dl-ai')

pinecone = Pinecone(api_key=PINECONE_API_KEY)

if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:
  pinecone.delete_index(INDEX_NAME)
pinecone.create_index(name=INDEX_NAME, dimension=256, metric='cosine',
  spec=ServerlessSpec(cloud='aws', region='us-east-1'))
index = pinecone.Index(INDEX_NAME)

"""Load the dataset"""

!wget -q --show-progress -O training.tar.zip "https://www.dropbox.com/scl/fi/rihfngx4ju5pzjzjj7u9z/lesson6.tar.zip?rlkey=rct9a9bo8euqgshrk8wiq2orh&dl=1"

!tar -xzvf training.tar.zip

!tar -xzvf lesson6.tar

!head -5 sample.log

!head -5 training.txt

"""Check cuda and set up the model


We are using bert-base-uncased sentence-transformers model that maps sentences to a 256 dimensional dense vector space.
"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=768)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())

model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model], device=device)
device

"""Train the Model"""

train_examples = []
with open('./training.txt', 'r') as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip()
        if line:
            a, b, label = line.split('^')
            train_examples.append(InputExample(texts=[a, b], label=float(label)))

#Define dataset, the dataloader and the training loss
warmup_steps=100
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
train_loss = losses.CosineSimilarityLoss(model)

import pickle
load_pretrained_model = False
if load_pretrained_model:
    trained_model_file = open('./pretrained_model', 'rb')
    db = pickle.load(trained_model_file)
    trained_model_file.close()
else:
    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=16, warmup_steps=100)

samples = []
with open('sample.log', 'r') as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip()
        if line:
            #emb = model.encode([line])
            samples.append(line)

"""Create Embeddings and Upsert to Pinecone"""

emb = model.encode(samples)

prepped = []
for i in tqdm(range(len(samples))):
  v = {'id':f'{i}', 'values':emb[i].tolist(), 'metadata':{'log':samples[i]}}
  prepped.append(v)
index.upsert(prepped)

"""Find the Anomaly"""

good_log_line = samples[0]

print(good_log_line)

results = []
while len(results)==0:  # After the upserts, it might take a few seconds for index to be ready for query.
    time.sleep(2)       # If results is empty we try again two seconds later.
    queried = index.query(
        vector=emb[0].tolist(),
        include_metadata=True,
        top_k=100
    )
    results = queried['matches']
    print(".:. ",end="")

for i in range(0,10) :
  print(f"{round(results[i]['score'], 4)}\t{results[i]['metadata']['log']}")

last_element = len(results) -1

print(f"{round(results[last_element]['score'], 4)}\t{results[last_element]['metadata']['log']}")