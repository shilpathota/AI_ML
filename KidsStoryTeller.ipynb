{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNshAM2fd21/qFf420SbCjb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shilpathota/AI_ML/blob/master/KidsStoryTeller.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Dependencies"
      ],
      "metadata": {
        "id": "fjUIlrkIDho6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fm5qswBWDRUH"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community huggingface_hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6HZOHxVFXSq",
        "outputId": "462eb289-94a4-4104-f68b-059096ef9264"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up hugging face Mistral LLM"
      ],
      "metadata": {
        "id": "WKwopYsjDmxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# üîê Replace with your Hugging Face token from https://huggingface.co/settings/tokens\n",
        "HUGGINGFACE_API_KEY = getpass.getpass(\"HUGGINGFACE_API_KEY\")\n",
        "client = InferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\", token=HUGGINGFACE_API_KEY)\n",
        "\n",
        "def call_mistral(prompt, temperature=0.7, top_p=0.95):\n",
        "    try:\n",
        "        result = client.text_generation(\n",
        "            prompt=prompt,\n",
        "            max_new_tokens=500,\n",
        "            stream=False,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True  # üëà ensures variation\n",
        "        )\n",
        "        return result if isinstance(result, str) else \"\".join(result)\n",
        "    except Exception as e:\n",
        "        return f\"[ERROR] Failed to generate story: {e}\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1DlK7BBDqK9",
        "outputId": "07f79fd7-7afb-4f50-f0fb-ff0340fffb68"
      },
      "execution_count": 124,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HUGGINGFACE_API_KEY¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "\n",
        "class StoryState(BaseModel):\n",
        "    profile: Optional[dict] = None\n",
        "    base_story: Optional[str] = None\n",
        "    final_story: Optional[str] = None"
      ],
      "metadata": {
        "id": "ub4jzs_JGjwb"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Agent Tools ‚Äì Profile, Story, Quiz, Feedback, Coordinator"
      ],
      "metadata": {
        "id": "h22BY_pLEH0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def node_load_profile(state: StoryState) -> StoryState:\n",
        "    try:\n",
        "        with open(\"profile.json\", \"r\") as f:\n",
        "            state.profile = json.load(f)\n",
        "    except:\n",
        "        state.profile = {\"name\": \"Chetu\", \"age\": 6, \"theme\": \"Paw Patrol\", \"math_level\": \"addition\"}\n",
        "    return state\n",
        "\n",
        "# Node: Story generation\n",
        "def node_story_agent(state: StoryState) -> StoryState:\n",
        "    profile = state.profile\n",
        "    prompt = f\"\"\"\n",
        "    Write a fun math story for a {profile['age']}-year-old named {profile['name']} who loves {profile['theme']}.\n",
        "    Focus on teaching {profile['math_level']}.\n",
        "    \"\"\"\n",
        "    state.base_story = call_mistral(prompt)\n",
        "    return state\n",
        "\n",
        "# Node: Quiz generation\n",
        "def node_quiz_agent(state: StoryState) -> StoryState:\n",
        "    story = state.base_story\n",
        "    profile = state.profile\n",
        "    prompt = f\"\"\"\n",
        "    Add 2 simple math questions into this story for a child (age {profile['age']}).\n",
        "    Return the story with questions inline, using this format:\n",
        "    Q: <question>\n",
        "    a) ...\n",
        "    b) ...\n",
        "    c) ...\n",
        "    Answer: <answer>\n",
        "\n",
        "    Story:\n",
        "    {story}\n",
        "    \"\"\"\n",
        "    state.final_story = call_mistral(prompt)\n",
        "    return state\n",
        "\n",
        "def node_feedback(state):\n",
        "    feedback = state.get(\"feedback\", {})\n",
        "    profile = state.get(\"profile\", {})\n",
        "    profile.update(feedback)\n",
        "    save_profile(profile)\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "wnscdMcAEHd6"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize LAngChain Agents"
      ],
      "metadata": {
        "id": "YqWme-nmEa6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph\n",
        "\n",
        "builder = StateGraph(StoryState)\n",
        "\n",
        "builder.add_node(\"load_profile\", node_load_profile)\n",
        "builder.add_node(\"story_agent\", node_story_agent)\n",
        "builder.add_node(\"quiz_agent\", node_quiz_agent)\n",
        "\n",
        "builder.set_entry_point(\"load_profile\")\n",
        "builder.add_edge(\"load_profile\", \"story_agent\")\n",
        "builder.add_edge(\"story_agent\", \"quiz_agent\")\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "jkX8bsQDEal5"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the flow"
      ],
      "metadata": {
        "id": "KvRaCG50GTsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = graph.invoke({})\n",
        "print(\"üîÅ Final Story with Quiz:\\n\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ngs4JhWE0D8",
        "outputId": "639d5ed0-f713-4bdc-c266-07b1c6786f81"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Final Story with Quiz:\n",
            "\n",
            "{'profile': {'name': 'Chetu', 'age': 6, 'theme': 'Peppa pig', 'math_level': 'addition'}, 'base_story': \"[ERROR] OpenAI call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\", 'final_story': \"[ERROR] OpenAI call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating UI"
      ],
      "metadata": {
        "id": "wbbr33HQMZNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n"
      ],
      "metadata": {
        "id": "Aiz-fUuxMpEC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def run_story_generator(name, age, theme, math_level):\n",
        "    # Save profile to JSON so LangGraph picks it up\n",
        "    profile = {\"name\": name, \"age\": age, \"theme\": theme, \"math_level\": math_level}\n",
        "    with open(\"profile.json\", \"w\") as f:\n",
        "        json.dump(profile, f)\n",
        "\n",
        "    result = graph.invoke({})  # Call your compiled LangGraph graph\n",
        "    return result[\"base_story\"]+result[\"final_story\"]\n"
      ],
      "metadata": {
        "id": "q3En7N37MssP"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üßÆ Personalized Math Story Generator\")\n",
        "\n",
        "    with gr.Row():\n",
        "        name = gr.Textbox(label=\"Child's Name\", value=\"Chetu\")\n",
        "        age = gr.Number(label=\"Age\", value=6)\n",
        "    theme = gr.Textbox(label=\"Favorite Theme\", value=\"Paw Patrol\")\n",
        "    math_level = gr.Dropdown(choices=[\"counting\", \"addition\", \"subtraction\", \"multiplication\"], label=\"Math Topic\", value=\"addition\")\n",
        "\n",
        "    generate_btn = gr.Button(\"üìñ Generate Story\")\n",
        "\n",
        "    output = gr.Textbox(label=\"Generated Story with Quiz\", lines=15)\n",
        "\n",
        "    generate_btn.click(fn=run_story_generator, inputs=[name, age, theme, math_level], outputs=output)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "8fRYUKa3MxZk",
        "outputId": "a7bda817-df62-4318-93ad-ebddc96b673c"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a5a83d1ec1cf34ea2d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a5a83d1ec1cf34ea2d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    }
  ]
}