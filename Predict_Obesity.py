# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mhnM_OkHzeBLrA6y8G1_x_8oUoTdBUhk
"""

# Importing the dataset
!pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition = fetch_ucirepo(id=544)

# data (as pandas dataframes)
X = estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.data.features
y = estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.data.targets

# metadata
print(estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.metadata)

# variable information
print(estimation_of_obesity_levels_based_on_eating_habits_and_physical_condition.variables)

print(X.shape)
print(X.dtypes)
print(X.head())
print(y.head())

"""Data Cleanup
- Removing Duplicates
- Encoding the Categorical Variables
- handling missing values
"""

import pandas as pd

# Dropping duplicates
df = pd.concat([X, y],axis=1)
df_cleaned = df.drop_duplicates(X)

X = df_cleaned.drop(columns = y.columns)
y = df_cleaned[y.columns]

# Count of missing values in each column of X
missing_X = X.isnull().sum()
print(y.isnull().sum())


# Display only columns with missing values
print(missing_X[missing_X > 0])

# For a combined DataFrame df
num_cols = df_cleaned.select_dtypes(include=['int64', 'float64']).columns
cat_cols = df_cleaned.select_dtypes(include=['object', 'category']).columns

# Filling the values
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(strategy='mean')  # or strategy='median'
df[num_cols] = num_imputer.fit_transform(df[num_cols])

# Categorical variables
cat_imputer = SimpleImputer(strategy='most_frequent')
df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])

# Label Encoder
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
for col in cat_cols:
    df[col] = le.fit_transform(df[col])

# Saving the model
import joblib

joblib.dump(cat_imputer, 'cat_imputer.pkl')
joblib.dump(le, 'label_encoder_gender.pkl')  # one per column if needed

print(df.info())
print(df.head())



"""| Column                                                           | Type            | Encoding Strategy                    | Notes                              |
| ---------------------------------------------------------------- | --------------- | ------------------------------------ | ---------------------------------- |
| `Gender`                                                         | Binary          | Already Encoded (0=female, 1=male)   | ‚úÖ Done                             |
| `Age`, `Height`, `Weight`                                        | Numerical       | No encoding needed                   | Just scale later                   |
| `family_history_with_overweight`, `FAVC`, `CAEC`, `SMOKE`, `SCC` | Binary/Nominal  | Already Encoded                      | ‚úÖ Done                             |
| `FCVC`, `NCP`, `CH2O`, `FAF`, `TUE`, `CALC`                      | Ordinal/Numeric | Keep as is or ensure float ‚Üí int     | Ordinal ‚Üí can be scaled            |
| `MTRANS`                                                         | Nominal         | Needs **One-Hot Encoding**           | Multi-class categorical, unordered |
| `NObeyesdad`                                                     | Target          | Keep as **Label Encoded** if already | Multiclass target: fine as numeric |

"""

# One-Hot Encode MTRANS properly
df = pd.get_dummies(df, columns=['MTRANS'], prefix='MTRANS', drop_first=True)

# Check the updated columns
print(df.columns)

print(df.info())
print(df.head())

# Check if we have all cleaned data
print(df[["Gender", "family_history_with_overweight", "FAVC", "CAEC", "SMOKE", "SCC"]].dtypes)
print(df[["Gender", "family_history_with_overweight", "FAVC", "CAEC", "SMOKE", "SCC"]].head())

"""Feature Scaling & Train test split"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. Separate features and target
X = df.drop("NObeyesdad", axis=1)
y = df["NObeyesdad"]

# 2. Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

"""Model Training

To choose best model I do stratified sampling where some strats are taken for each of the K fold cross validation
"""

from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
from xgboost import XGBClassifier

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier()
}

# Cross-validation setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
results = {}

for name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    results[name] = scores
    print(f"{name}: Mean Accuracy = {scores.mean():.4f} | Std = {scores.std():.4f}")

"""The best model is Random Forest, XGBoost and SVM which has 94%, 97% and 85% accuracy

Hyper parameter tuning using Grid Search CV using the hyper parameters n_estimators, max_depth, min_samples_split
"""

# Top 1 - Random Forest
from sklearn.model_selection import GridSearchCV

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_rf.fit(X_train, y_train)

print("Best RF Params:", grid_rf.best_params_)
print("Best RF Accuracy:", grid_rf.best_score_)

# Top 2 - SVM
param_grid_svm = {
    'C': [10, 11, 9],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

grid_svm = GridSearchCV(SVC(), param_grid_svm, cv=5, scoring='accuracy', n_jobs=-1)
grid_svm.fit(X_train, y_train)

print("Best SVM Params:", grid_svm.best_params_)
print("Best SVM Accuracy:", grid_svm.best_score_)

# Xgboost
param_grid_xgb = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'gamma': [0, 1],
    'reg_lambda': [1, 10]  # L2 regularization
}
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

grid_xgb = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid_xgb,
    scoring='accuracy',
    cv=5,
    verbose=1,
    n_jobs=-1
)

grid_xgb.fit(X_train, y_train)

print("‚úÖ Best Parameters:", grid_xgb.best_params_)
print("üèÜ Best CV Accuracy:", grid_xgb.best_score_)

from sklearn.metrics import classification_report, confusion_matrix

#best_model = grid_rf.best_estimator_
best_model =  grid_svm.best_estimator_

y_pred = best_model.predict(X_test)

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Evaluating Xgboost
from sklearn.metrics import classification_report, confusion_matrix

best_xgb = grid_xgb.best_estimator_
y_pred_xgb = best_xgb.predict(X_test)

print("üìä Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))
print("\nüìù Classification Report:\n", classification_report(y_test, y_pred_xgb))

"""SAving the model that is best"""

import joblib

joblib.dump(best_model, 'best_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
X = df.drop("NObeyesdad", axis=1)
joblib.dump(X.columns.tolist(), "feature_columns.pkl")

"""Loading the model to predict"""

import joblib
import pandas as pd
import numpy as np

# Load model, scaler, and column names
model = joblib.load("best_model.pkl")
scaler = joblib.load("scaler.pkl")
feature_columns = joblib.load("feature_columns.pkl")

# Sample input for prediction
sample_input = [[1, 25, 1.70, 65, 1, 1, 2.0, 3.0, 1, 0, 2.0, 1, 2.0, 1.0, 2, 0, 1, 0, 0]]  # e.g., "bike" as MTRANS
new_data = pd.DataFrame(sample_input, columns=feature_columns)

# Scale and predict
features_scaled = scaler.transform(new_data)
prediction = model.predict(features_scaled)
print("‚úÖ Predicted class:", prediction[0])

"""DEploying the model using flask"""

from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)
model = joblib.load('best_model.pkl')
scaler = joblib.load('scaler.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    input_data = request.get_json(force=True)
    features = np.array(input_data['features']).reshape(1, -1)
    features_scaled = scaler.transform(features)
    prediction = model.predict(features_scaled)
    return jsonify({'prediction': int(prediction[0])})

if __name__ == '__main__':
    app.run(debug=True)