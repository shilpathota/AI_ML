{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2krcajnChjq7NndKZU1Up",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
   
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shilpathota/AI_ML/blob/master/Text_To_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic LoRA Technique"
      ],
      "metadata": {
        "id": "hqzY7ETnLmOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4MwRqMUBEVhX"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"transformers>=4.40.0\" \"datasets>=2.18.0\" \"accelerate>=0.30.0\" \"peft>=0.11.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a tiny dataset and baseline model"
      ],
      "metadata": {
        "id": "n9w-_S5CLwq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Tiny subset for demo (100 train / 50 test)\n",
        "ds = load_dataset(\"imdb\")\n",
        "small_train = ds[\"train\"].shuffle(seed=42).select(range(200))\n",
        "small_test  = ds[\"test\"].shuffle(seed=42).select(range(50))\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560,
          "referenced_widgets": [
            "bd2dedd112b64dba84764dd99f8fb3c4",
            "c40d8424297f4f5eb6403d4115ad4cce",
            "1d9d8d414efb45e6b824fb6c448e6c86",
            "431df8ccff0040b0a93d54740f33129d",
            "65dd1f56f8fb4bd79ddbc10a7d7eb1ac",
            "3802d28715264e1b960c1a7ab92b360d",
            "04661235b7044eec9f64935fec7fe0d7",
            "856583e55382451398aa0dd61644f55a",
            "737a5bae01b549b1ae4b1eae0253320b",
            "8da509b61f8847b7aca8906fb582cd7c",
            "5cb2583c48ce4e4e8cc3efb123a718f6",
            "500fc14ccbe24a5197c0b0c7916dcc80",
            "552ae01111144b4ab53a3d3cb182dc40",
            "e1f7601b8f1b4d2eaa9b12268619c64c",
            "07af4242a1894be6b6db58d75f6a50c7",
            "bb4516f94a4c478495090cd3cd17fbd3",
            "edc0a8ccc2f745c8b35b6d7b9dc65f65",
            "fff29cd2eb92480b829b23340ae5e551",
            "677e275aecae48f8997b4affe316e0e5",
            "b9307d77f9544efa91e1ed2d44883f9d",
            "9b6670452b9547e68b68ec8553d0dda5",
            "dc53844e48a84d378e9b194b5ffba5f3",
            "5b5e606c51a54a408dea3218a9f54eb6",
            "4f247062e3854c9fb61709094cdeae1f",
            "d496f52ab524440a9932a13d4eeed958",
            "a85dc3019ec64f6fa0e630005d4e8f6f",
            "88ba2501653c4b64bbc3eb465a346a63",
            "73ce8fc18b1043dd87ed0ed7b875908e",
            "e55e9722095f4c50a41b636a156993b0",
            "9a6c8157917d4465bd42b81f4c49bcf6",
            "4dc12b8faf334bf0882cf06b17ca0185",
            "5eb7d65bfea8476ca22936e0deecc553",
            "a8b63906a7934af0adecdfbefed47bc4",
            "76d368a62dec44c9bf1fe7c02ffdb3b5",
            "b2343af13d8d45e7a87dd4fa907dd024",
            "8f9bcfb51f564a4697a4925cf458559d",
            "d9b52e48f4394eaaab54f8784ba4f4d5",
            "11e7921975d84293afbae58ffd5d1852",
            "1622bf83c14f4d8fb0cc5b3194f57b2e",
            "9a1575a61ec44d97823c81c13ad2c936",
            "382b816100524973abedbbdf13668d34",
            "01c1ab14abc8465aad6601da8e4ca031",
            "e592e9c0f1fc42ec8648179582da3796",
            "f7c6d174cb9a4830b8ea33222f32c900",
            "98d5b8aff62e4d5192bafe5006fb2d65",
            "02b60d8041934197a7af0ae5f7387c44",
            "5c6c78d885724ed79c88613ffdee9e93",
            "dcbc2fc7c9a441da827d19a432824ab1",
            "1ad436e963964c1a89280916815f1994",
            "63b45919b5684852858dbf72100a3e50",
            "5f6c8e4d4cb047378fee393d26ea7e2a",
            "f21d788aedcb404cbe5337ff4ca8b85a",
            "cf7fccda7a5d4cbf9c09c56db61b9983",
            "059749e88eb64aa9a0ba3ff69460aa36",
            "415982da77ff49e185ff61bd712be01d",
            "03db7a92bb994c49a978a25ef89e659b",
            "ffafb421fac44dd992840e35307e399a",
            "6dc8edf06ed1415a8832c3178d714365",
            "f46acd6cee4b46a58f8e86b43c4cf801",
            "14f03c92dc2449bba5461294c14c8f7c",
            "2a0772bc0bae4c938078b8d98b57292a",
            "dd866d4a8ffa4a9880119dc6f13ed603",
            "2e422dceeb514abebe3d50998a4e4103",
            "d265131a69764f97875201bb74175c18",
            "468e17e950554e62bb51aaed9299d497",
            "cbd4221b904d4ce9a0cacf11af742751",
            "77f1552d5a6d4fe68caee455aec1b71e",
            "3e0d59c4714b461fb11049e69acb637f",
            "14bf8c1e041f4af79c2d8425c5e9ffe2",
            "72a670410a134aaab110ec2ca35197b0",
            "752c6cbcc9e44892bd3e6b090a1d17c1",
            "938075b6d69b47199d079881a33ebbc6",
            "563df13bfd1849e99492dfa16ee93eb3",
            "71f8952a680343298fa9e88aa071ad8d",
            "8b3f5de42cdc4bde931f280652f0340a",
            "0a1d605e4bf04ea38db7d287f6010f11",
            "fe1489ba73944cdca27891d9cb7c1bbd",
            "08653e89714c4e96bb72f3064da518dc",
            "31f970fe63814f05b695ae854af30d73",
            "7f901efa8274480899b82f3c89511aae",
            "de9461b9e64f469f81da126695815c3e",
            "ebd6191e076840038795695be93342a0",
            "dcc367e3808447ce871d9f0ff147fed4",
            "dc97644fe23a465facfb667354346620",
            "29ee5035aabf4b5589e42ce41c666c5b",
            "2f2f2a50b8ff4682b13a562b71aa0abb",
            "aeb121c9450044399743f3f0a5ad5aa4",
            "2853f8a6721946d88d9c1e9249789514",
            "d77db999e14345c4b1531cf0ae0e8c8c",
            "bbfc1b057884416eb9831937c5dc2837",
            "ae01fb3b13ec482cae616cef1391a3fd",
            "5964fcf6950145bba5340b11750a9b5e",
            "d2c5e7b289b54b0bbeea95cd79dacc76",
            "0a138dfda24c40dd9fabd3f33502c0dd",
            "e68fcb688e054d3dae7c1b5ee61bd74f",
            "2ccacc3879bb4dc496d06521385ace97",
            "8a5cd06734e54c4c976425ef36b068ee",
            "b992b7eda21f47128eff82f90deb4fc3",
            "727cade162b74b599bb67fc79ebb2644",
            "53d374aec235427ea38eabfe6ed78110",
            "f8583532719f4a5cb2175968ea382a2b",
            "29424410439442f2bfb3bea424eb4693",
            "90c0e38b488f44f1b1c9af6351f00148",
            "b358b780317c47b39b8f0f85965ec1d9",
            "8af4adc6b9984f05baef5d10f0d8c870",
            "8e9be78c5cbf415f9d84535098873fe8",
            "b69c153fb1744cae95dbc232496ec4d6",
            "aa62888ab8e1409daba62294130454cd",
            "ad248bd3980f4101b3db88718731c917",
            "a5a3d6a74aa647148fdd5173a330c018",
            "5ff451d898904a9eb68c1de4213a3fd8",
            "863d74ac762a4d6dab81be06ee7a5fd5",
            "db65ea8843e14bcbac73b0b7baeee348",
            "9357fd93b8dd4ea987251c9e87bf2149",
            "411d7430a55b467686c7c3b7b092c012",
            "8e68b110a09747a19f42ae1f621ac760",
            "b709753fb99242c989e7c25186509fc2",
            "b58ff4311a904accb884e1e5203f74a4",
            "1a05015a50904f33b8503dbc64cb858f",
            "fbce661f557b4439af6df706bfa2755c",
            "7d04ea683bf145d7bf05583ff3cc8fd8",
            "1e05976076ed452c8f714b0d5bdd0c02",
            "9dbcd4c61ab646349f73440b1223e410",
            "4fca9d2532a74e8f800ff4c5778bba92",
            "44fae57679b4458ab49b116529471d76",
            "978b9eee504e44f1bdd657e9ec9c847e",
            "4bc4e00153bd4979b81e2ca01df43e10",
            "026057407e5a47348c7db7eb62a615fc",
            "f2d5ac55829d4e58abfdf3caec034d36",
            "b812caccb18f40d2a6bb7f09434b861a",
            "746ab1ca89704113b2d0be621c7f81cc",
            "62788061a5f140eaa26be4f1d604997c"
          ]
        },
        "id": "Q6owPQN4EbUm",
        "outputId": "3cdfa050-f7ae-49d1-bd73-94d1ce254be9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd2dedd112b64dba84764dd99f8fb3c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "500fc14ccbe24a5197c0b0c7916dcc80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b5e606c51a54a408dea3218a9f54eb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "plain_text/unsupervised-00000-of-00001.p(â€¦):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76d368a62dec44c9bf1fe7c02ffdb3b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98d5b8aff62e4d5192bafe5006fb2d65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03db7a92bb994c49a978a25ef89e659b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77f1552d5a6d4fe68caee455aec1b71e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08653e89714c4e96bb72f3064da518dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d77db999e14345c4b1531cf0ae0e8c8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53d374aec235427ea38eabfe6ed78110"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ff451d898904a9eb68c1de4213a3fd8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e05976076ed452c8f714b0d5bdd0c02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick baseline inference (before LoRA)"
      ],
      "metadata": {
        "id": "HH7-jQ7GL9--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sent(text, model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    probs = logits.softmax(dim=-1).cpu().numpy()[0]\n",
        "    return {\"neg\": float(probs[0]), \"pos\": float(probs[1])}\n",
        "\n",
        "sample_text = \"The movie was surprisingly good and I loved the acting!\"\n",
        "print(\"Baseline:\", predict_sent(sample_text, base_model))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zglsqqBEf3H",
        "outputId": "7eae8de6-b7f2-4456-dfb2-1dc4cca11018"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: {'neg': 0.5112807154655457, 'pos': 0.48871931433677673}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrap the model with LoRA and fine-tune"
      ],
      "metadata": {
        "id": "2a4_wLQnMB5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],  # DistilBERT attention module names\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\",\n",
        ")\n",
        "\n",
        "model_lora = get_peft_model(base_model, lora_config)\n",
        "model_lora.print_trainable_parameters()\n",
        "\n",
        "def encode_batch(batch):\n",
        "    enc = tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "    )\n",
        "    enc[\"labels\"] = batch[\"label\"]\n",
        "    return enc\n",
        "\n",
        "train_enc = small_train.map(encode_batch, batched=True)\n",
        "test_enc  = small_test.map(encode_batch, batched=True)\n",
        "\n",
        "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "train_enc.set_format(type=\"torch\", columns=columns)\n",
        "test_enc.set_format(type=\"torch\", columns=columns)\n",
        "\n",
        "train_loader = DataLoader(train_enc, batch_size=8, shuffle=True)\n",
        "test_loader  = DataLoader(test_enc, batch_size=8)\n",
        "\n",
        "optimizer = AdamW(model_lora.parameters(), lr=2e-4)\n",
        "num_epochs = 1\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * num_training_steps),\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "model_lora.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model_lora(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        if step % 20 == 0:\n",
        "            print(f\"Epoch {epoch} Step {step} Loss {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222,
          "referenced_widgets": [
            "b6bb7ae531664713a2fa3c855f4a939b",
            "6830aa8869004c18b7ccdf35906a1514",
            "a6962d129a2d47e0b806bb7283371f7c",
            "1ec8d47932c9435c9b778109dad70cd2",
            "b739ba94f2924be7bc09c4a7ae6e6538",
            "eb3d9252e99340cea0476abc205e8aac",
            "96f5a434466c4846bccb56c985dae85c",
            "2aed46b4247d4f82837b61c656077a60",
            "fc2a6408f9ae4db483cf11abeaef9884",
            "431c82f7abf644578b7508c782ed5994",
            "09be802065c64f67953450018a90e810",
            "b8b3f461d2544856b0af526a4ce9f4f2",
            "7e5a8d52897d48c3a370b45b57190c79",
            "49ac0fc83230450cbaf8bc68b799f7ee",
            "41de78259ae74547b8451db0ed5b19f5",
            "75a1c7339e9c4d85890804731ca10cc3",
            "a4692b7f33b84453b4fa05a2dffe8f96",
            "9e2811dd1a7e43218e2ffb68f47d26ef",
            "bb0cf881aa4d466f8f7226fc08300787",
            "030fb0f13c2b49d898ecce1a329022ed",
            "067b618e66134c598428f0d7ac838f25",
            "ea0d8542726f4e52add0ae8407796c53"
          ]
        },
        "id": "IbGckCIfEkWj",
        "outputId": "83879ddb-e64f-4364-8204-0b48601b742c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 887,042 || all params: 67,842,052 || trainable%: 1.3075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6bb7ae531664713a2fa3c855f4a939b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8b3f461d2544856b0af526a4ce9f4f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Step 0 Loss 0.6478\n",
            "Epoch 0 Step 20 Loss 0.6956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check improvement (after LoRA)"
      ],
      "metadata": {
        "id": "Nbvl6nJHMStW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lora.eval()\n",
        "\n",
        "def accuracy(dataloader, model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            logits = model(**batch).logits\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        correct += (preds == batch[\"labels\"]).sum().item()\n",
        "        total += preds.size(0)\n",
        "    return correct / total\n",
        "\n",
        "print(\"Test accuracy with LoRA:\", accuracy(test_loader, model_lora))\n",
        "\n",
        "print(\"Before vs After on a sample:\")\n",
        "print(\"LoRA output:\", predict_sent(sample_text, model_lora))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEyE5NUSEou2",
        "outputId": "663fd175-6cb4-4717-ba35-15955953c115"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with LoRA: 0.46\n",
            "Before vs After on a sample:\n",
            "LoRA output: {'neg': 0.4301743507385254, 'pos': 0.5698257088661194}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mimic Text-to-LoRA paper"
      ],
      "metadata": {
        "id": "Fup5p_6oMZM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers peft torch\n"
      ],
      "metadata": {
        "id": "kaRDfZe4JaBu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install & load base model"
      ],
      "metadata": {
        "id": "uyPKURrjMeYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_id = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "base_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709,
          "referenced_widgets": [
            "f142e288a0394aa083ccb4332cc6d26b",
            "df2840baeb104948986ec6d02167d15e",
            "579b8c1c6885447193d34700ab0248c7",
            "3867bc96adf64ba9aac30ca76c43cb90",
            "deecaeebd2a64a4c9b2de08b79f2ec4d",
            "341cf23b1def4b40bf9c880f6cd7ce39",
            "a188f9358abd4bd182796dd5c0f8d0ff",
            "ba04836695f140e58cdbf43cd5513600",
            "3c604adeba984eb595109c2b3c09d2c3",
            "12ec9e8080224462b42516ff6369ce78",
            "38d2618e9bba4c3180e07428acb3b7ba",
            "d9be3a5bde4347b186365ed14aa015ce",
            "61f8d23ba9774842aff34b03c114ced0",
            "a1e7e3bd7d7648a69d2eee7c4f7be06f",
            "09f522366de8440297439758088daa9c",
            "82771537b4664703963db62c0dfb4d8c",
            "afdceced7da14e4e9ccaf598e2e9ebdc",
            "97a6d54f99b14f8987cc8999f67d0f47",
            "92452078082c41fbad7de2d4380f12a6",
            "c63e2bc1f4264dd5ac3516d41d939c24",
            "e2cc3fc0b5db4937907ed0ab98f38a4d",
            "40f2ab109a6d4421bcb3477d841cde9a",
            "ef375038c901462c934ee5adde4464ea",
            "917405f15b094dbf91a6ecfee360a6e0",
            "bf12fb3742794da09cb2ff54a86f8af9",
            "d870a189c842404f8362f73f1207c71c",
            "72ec7009ec7049dba48445184a896be2",
            "7268a71854514d3d8a2119c6dbb015a4",
            "41b2fa18e5dc4d73a4cc1d8cdb5b41fb",
            "8c07cddaa3574bb0b2aec470178f956c",
            "47ae42eab7a446afabb9211b23854e70",
            "ce3b7e7e6023433db0df040e1fcb5fd2",
            "521ec2f9466c48c9bba1a2dd1df3c125",
            "edadc61e84f44ede917761fcb6918a6f",
            "6f10b4e3d8dd417e8a4a4ea639bb3a81",
            "1e82d19f5c87442cb67b25b88f6d545e",
            "f1fadd4ad4fc4889b703803ae411bd1a",
            "db7fcbfb8e554ee1b253bf08b171e264",
            "af2336b30dc840eba2cc1bb98dc2a50d",
            "8e984e4811dc4ca7a4a676fa66af6b05",
            "48e6990730b34798a98f2fe348f23754",
            "19281ac8016444cf9c4fda005779ea6e",
            "d728fc6a9789400690326cddd2b830da",
            "362368801d5444cf9347da54a05f8eb8",
            "1afece6756594023ab2f55d28fe56d62",
            "9aaf853b63ad4b66aad45031603527d4",
            "5dc639fcbb2b4bd3b828130d2d1e5845",
            "40ea690af4654388bd416b8fbed81034",
            "bfabd7957e1a4ea69635b67a279cd37d",
            "6ca5ff2a118b42ae805097050905caea",
            "b219f348d66844feaaf064418f8d6ca9",
            "cde34f6211604cd6b9014f5ac1191848",
            "f92c952732b34680a1e19c854567cdc5",
            "846a0f4a69f046e5b884c96340c1da69",
            "910a82d467cb46cfbeb7e3c2b835c9fb",
            "7dd5cc9ffe674a0dab96c407e6d379bd",
            "71847b0c5d41469482a79bf6d5530370",
            "2de9960089624f02a1ae5dd80ad648d0",
            "15e56dd60283464dae9287df4da0ca4b",
            "f3387af16ac24acfa419d30453a59046",
            "0da20308e37e47b69593dc8e54de09a1",
            "c5f3f315ecd24159880bb031038af664",
            "e430d706c9c04dad910ebfd3b210899e",
            "33e38e5198924e16a4154c0e25a7059c",
            "9177725f9a024c0ba464bc8f35650301",
            "1f50194047964ffd8eddeadde106af4c",
            "0ff8378f375e4ea9b81e2fad336c20d0",
            "0983d577d76a477f84ac5b64798b90ad",
            "5f30e090731a4c53bdd3e69263b497c0",
            "a204e18fe0674ef88c5d1f27f0c8d7b9",
            "3edb5a4795e348f3b4c55819d7196e1c",
            "4e362a3dc2454213b3de6621c941f20b",
            "bbae4be0c84e4976bfab0751d2d44c12",
            "873cd3f27d204a108108cee748b9c049",
            "2dcc0f28235f41bab0c10a03ce0d9e73",
            "339883da5f2640ea9f387c43034ab984",
            "0bd93be88505420cb736884ca6c85ebd"
          ]
        },
        "id": "RjrUeo4ZJbH-",
        "outputId": "8813ed5c-4657-4425-d0f2-cbe769b9691e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f142e288a0394aa083ccb4332cc6d26b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9be3a5bde4347b186365ed14aa015ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef375038c901462c934ee5adde4464ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edadc61e84f44ede917761fcb6918a6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1afece6756594023ab2f55d28fe56d62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dd5cc9ffe674a0dab96c407e6d379bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ff8378f375e4ea9b81e2fad336c20d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attach LoRA (untrained)"
      ],
      "metadata": {
        "id": "YDBmaP3gNKzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_cfg = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(base_model, lora_cfg).to(device)\n",
        "lora_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcXX-zEHJgjy",
        "outputId": "b4a020e1-68de-458f-ea2b-2ec43760f9bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GPT2LMHeadModel(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(50257, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0-11): 12 x GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2Attention(\n",
              "              (c_attn): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=2304, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (c_proj): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=768, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=768, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D(nf=3072, nx=768)\n",
              "              (c_proj): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=768, nx=3072)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=768, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode text (simple version)"
      ],
      "metadata": {
        "id": "BAkZqnC_M_k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text(text):\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        emb = base_model.transformer.wte(tokens[\"input_ids\"])\n",
        "    return emb.mean(dim=1)  # [1, hidden_dim]\n"
      ],
      "metadata": {
        "id": "0gm1NkbhJlV7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a hypernetwork and Generate & inject LoRA weights"
      ],
      "metadata": {
        "id": "hLqxvy_9M4Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "hidden = base_model.config.n_embd\n",
        "r = 4\n",
        "\n",
        "# Hypernet outputs \"base\" LoRA in hidden space\n",
        "class LoRAHyperNet(nn.Module):\n",
        "    def __init__(self, text_dim, hidden_dim, r):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(text_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 2 * hidden_dim * r)\n",
        "        )\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.r = r\n",
        "\n",
        "    def forward(self, text_emb):\n",
        "        out = self.net(text_emb)                    # [1, 2*hidden*r]\n",
        "        out = out.squeeze(0)\n",
        "        A_flat, B_flat = torch.split(out, self.hidden_dim * self.r)\n",
        "        A_base = A_flat.view(self.r, self.hidden_dim)       # (r, hidden)\n",
        "        B_base = B_flat.view(self.hidden_dim, self.r)       # (hidden, r)\n",
        "        return A_base, B_base\n",
        "\n",
        "hypernet = LoRAHyperNet(hidden, hidden, r).to(device)\n",
        "\n",
        "# Cache fixed random projections so shapes are stable\n",
        "proj_in_cache = {}\n",
        "proj_out_cache = {}\n",
        "\n",
        "def get_proj_in(in_features):\n",
        "    if in_features not in proj_in_cache:\n",
        "        g = torch.Generator(device=device)\n",
        "        g.manual_seed(1234 + in_features)\n",
        "        # (hidden, in_features)\n",
        "        proj_in_cache[in_features] = torch.randn(hidden, in_features, generator=g, device=device) / (hidden ** 0.5)\n",
        "    return proj_in_cache[in_features]\n",
        "\n",
        "def get_proj_out(out_features):\n",
        "    if out_features not in proj_out_cache:\n",
        "        g = torch.Generator(device=device)\n",
        "        g.manual_seed(5678 + out_features)\n",
        "        # (out_features, hidden)\n",
        "        proj_out_cache[out_features] = torch.randn(out_features, hidden, generator=g, device=device) / (hidden ** 0.5)\n",
        "    return proj_out_cache[out_features]\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_text_mean(text: str):\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    # token embeddings: [1, seq, hidden]\n",
        "    emb = base_model.transformer.wte(tokens[\"input_ids\"])\n",
        "    return emb.mean(dim=1)  # [1, hidden]\n",
        "\n",
        "@torch.no_grad()\n",
        "def apply_hyper_lora(model, description: str, scale=0.05):\n",
        "    text_emb = encode_text_mean(description)\n",
        "    A_base, B_base = hypernet(text_emb)\n",
        "\n",
        "    # Optional: keep LoRA magnitudes small\n",
        "    A_base = scale * A_base\n",
        "    B_base = scale * B_base\n",
        "\n",
        "    for module in model.modules():\n",
        "        if hasattr(module, \"lora_A\") and hasattr(module, \"lora_B\"):\n",
        "            for k in module.lora_A.keys():\n",
        "                A_w = module.lora_A[k].weight   # (r, in_features)\n",
        "                B_w = module.lora_B[k].weight   # (out_features, r)\n",
        "\n",
        "                in_features = A_w.shape[1]\n",
        "                out_features = B_w.shape[0]\n",
        "\n",
        "                R_in = get_proj_in(in_features)       # (hidden, in_features)\n",
        "                R_out = get_proj_out(out_features)    # (out_features, hidden)\n",
        "\n",
        "                A = A_base @ R_in                     # (r, in_features)\n",
        "                B = R_out @ B_base                    # (out_features, r)\n",
        "\n",
        "                A_w.copy_(A)\n",
        "                B_w.copy_(B)\n"
      ],
      "metadata": {
        "id": "LP4WYCGoKIpQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Outputs of Base Model and LoRA Model"
      ],
      "metadata": {
        "id": "4nLMJhMvMx6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apply_hyper_lora(lora_model, \"Answer formally like a science professor.\")\n",
        "\n",
        "def generate(model, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    out = model.generate(**inputs, max_new_tokens=60)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "prompt = \"Why is the sky blue?\"\n",
        "\n",
        "print(\"Base model:\")\n",
        "print(generate(base_model, prompt))\n",
        "\n",
        "print(\"\\nBase + Hypernetwork-generated LoRA:\")\n",
        "print(generate(lora_model, prompt))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-QbjYmTKKKD",
        "outputId": "40c6403c-3fe2-44e8-d2fa-380bc495c55a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why is the sky blue?\n",
            "\n",
            "The sky blue is a color that is often associated with the sky. It is a color that is often associated with the sky. It is a color that is often associated with the sky. It is a color that is often associated with the sky. It is a color that is often associated\n",
            "\n",
            "Base + Hypernetwork-generated LoRA:\n",
            "Why is the sky blue?\n",
            "\n",
            "The sky blue is a color that is often associated with the sky. It is a color that is often associated with the sky. It is a color that is often associated with the sky. It is a color that is often associated with the sky. It is a color that is often associated\n"
          ]
        }
      ]
    }
  ]
}
